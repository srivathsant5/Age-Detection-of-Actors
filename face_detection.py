# -*- coding: utf-8 -*-
"""Face Detection of the actors.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13_z9-gTEuzA8tUWzbME4nc4uC6fyTIOY

# Face Detection of the actors
"""

#impoeting libraries 
import numpy as np # linear algebra
import pandas as pd # data processing,

import pandas as pd
import numpy as np
import tensorflow as tf
import glob
import os
import seaborn as sns 
from matplotlib import pyplot as plt
from sklearn import metrics
from sklearn.metrics import confusion_matrix

import warnings
warnings.filterwarnings('ignore')

from google.colab import drive
drive.mount('/content/drive')

# read the csv file to check out the images and class
data = pd.read_csv("/content/drive/MyDrive/DSP/Analytical  Vidhya/1 - Face detection/Dataset/train.csv")
data.head()

data['Class'].replace(['YOUNG','MIDDLE','OLD'],[0,1,2],inplace=True)
data.head(3)

def readImage(path,ch = 3, resize=(150,150)):
	di = tf.io.read_file(path)
	di = tf.image.decode_jpeg(di, channels=ch)
	di = tf.image.convert_image_dtype(di, dtype=tf.float32)
	di = tf.image.resize(di, resize)
	return di

# function to load the data that includes images and respective labels
def load_data(image_path, label):
    img = readImage(image_path, 3, (150,150))
    return (img, label)

# built the list of image paths and list of respective responses of the images
PATH = "/content/drive/MyDrive/DSP/Analytical  Vidhya/1 - Face detection/Dataset/Train/final/Train"
image_paths = []
for path in os.listdir(PATH):
    image_paths.append(PATH+"/"+path)
print(len(image_paths))

response_list = []

for i in image_paths:
    _,tail = os.path.split(i)
    response = data.loc[data['ID'] == tail]['Class'].values[0]
    response_list.append(response)
print(len(response_list))

response_list

# split the dataset into train and test dataset
train_size = int(0.9*(len(image_paths)))
print(train_size)
test_size = int(0.1*(len(image_paths)))

train_set = tf.data.Dataset.from_tensor_slices((image_paths[:train_size], response_list[:train_size]))
test_set = tf.data.Dataset.from_tensor_slices((image_paths[test_size:], response_list[test_size:]))

print("train dataset size - ", train_size)
print("test dataset size - ", test_size)

train_set = (train_set
    .map(load_data, num_parallel_calls=tf.data.AUTOTUNE)
    .batch(64)
    .prefetch(tf.data.AUTOTUNE)
)

test_set = (test_set
    .map(load_data, num_parallel_calls=tf.data.AUTOTUNE)
    .batch(64)
    .prefetch(tf.data.AUTOTUNE)
)

# build the layers of CNN model
from tensorflow.keras import layers,models

cnn_model = models.Sequential([
    layers.Conv2D(filters=30, kernel_size=(3, 3), activation='relu', input_shape=(150, 150, 3), padding = 'same'),
    layers.MaxPooling2D((2, 2)),
    
    # layers.BatchNormalization(),

    layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding = 'same'),
    layers.MaxPooling2D((2, 2)),

    # layers.BatchNormalization(),
    
    # layers.Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding = 'same'),
    # layers.MaxPooling2D((2, 2)),
    
    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    # layers.Dropout(0.25),
    layers.Dense(3, activation='softmax')
])

# view the summary of the cnn model
cnn_model.summary()

# compile the model
cnn_model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# fit the model
cnn_model.fit(train_set, epochs=10, validation_data=test_set)

#train accuracy 
cnn_model.evaluate(train_set)

test_set

# test accuracy
cnn_model.evaluate(test_set)

test_pred = cnn_model.predict(test_set)

y_labels = [np.argmax(item) for item in test_pred]
print("Test Predictions response sample:",y_labels[:10])

test_response = response_list[test_size:]
print("Test True response sample:", test_response[:10])

class_names = ['YOUNG','MIDDLE','OLD']

# funtion to plot confusio matrix to check the accuracy of each class value
def plot_confusion_matrix(y_true, y_pred, classes,
                          normalize=False,
                          title=None,
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    if not title:
        if normalize:
            title = 'Normalized confusion matrix'
        else:
            title = 'Confusion matrix, without normalization'

    # Compute confusion matrix
    cm = confusion_matrix(y_true, y_pred)
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

#     print(cm)

    fig, ax = plt.subplots(figsize=(7,7))
    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)
    ax.figure.colorbar(im, ax=ax)
    # We want to show all ticks...
    ax.set(xticks=np.arange(cm.shape[1]),
           yticks=np.arange(cm.shape[0]),
           # ... and label them with the respective list entries
           xticklabels=classes, yticklabels=classes,
           title=title,
           ylabel='True label',
           xlabel='Predicted label')

    # Rotate the tick labels and set their alignment.
    plt.setp(ax.get_xticklabels(), rotation=45, ha="right",
             rotation_mode="anchor")
    # Loop over data dimensions and create text annotations.
    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            ax.text(j, i, format(cm[i, j], fmt),
                    ha="center", va="center",
                    color="white" if cm[i, j] > thresh else "black")
    fig.tight_layout()
    return ax


np.set_printoptions(precision=2)

# plotting confusion matrix without normalization
plot_confusion_matrix(y_labels, test_response, classes=class_names,
                      title='Confusion matrix, without normalization')

# plotting confusion matrix with normalization
plot_confusion_matrix(y_labels, test_response, classes=class_names, normalize=True,
                      title='Normalized confusion matrix')

"""# Prediction for test dataset using final model"""

#Lets load the test data set
test = pd.read_csv('/content/drive/MyDrive/DSP/Analytical  Vidhya/1 - Face detection/test_dataset2/test_Bh8pGW3/test.csv')
test.head()

#data['Class'].replace(['YOUNG','MIDDLE','OLD'],[0,1,2],inplace=True)
#data.head(3)

def readImage(path,ch = 3, resize=(150,150)):
	di = tf.io.read_file(path)
	di = tf.image.decode_jpeg(di, channels=ch)
	di = tf.image.convert_image_dtype(di, dtype=tf.float32)
	di = tf.image.resize(di, resize)
	return di

# built the list of image paths and list of respective responses of the images
PATH = "/content/drive/MyDrive/DSP/Analytical  Vidhya/1 - Face detection/test_dataset2/test_Bh8pGW3/Test"
image_paths = []
for path in os.listdir(PATH):
    image_paths.append(PATH+"/"+path)
print(len(image_paths))

#response_list = []

#for i in image_paths:
 #   _,tail = os.path.split(i)
    #response = test.loc[test['ID'] == tail]['Class'].values[0]
   # response_list.append(response)
#print(len(response_list))

image_paths

test_data = tf.data.Dataset.from_tensor_slices(image_paths[:])

test_data

# function to load the data that includes images and respective labels
def load_data(image_path):
    img = readImage(image_path, 3, (150,150))
    return img

test_data = (test_data
    .map(load_data, num_parallel_calls=tf.data.AUTOTUNE)
    .batch(64)
    .prefetch(tf.data.AUTOTUNE)
)

test_data

pred = cnn_model.predict(test_data)

pred[0]

predictions = []
for i in pred:
  predictions.append(np.argmax(i))

predictions[0:10]

pred = pd.DataFrame(predictions, columns = ['Class'])
pred

x_test = pd.concat([test,pred], axis=1)
x_test

x_test.Class.value_counts()

x_test.dtypes

x_test['Class'] = x_test['Class'].replace({0: 'YOUNG' ,
                                             1: 'MIDDLE',
                                             2:'OLD'})

x_test.head(10)

#saving the data into csv file
x_test.to_csv(r"solution_submission.csv")

"""# **Thank you !!!**"""